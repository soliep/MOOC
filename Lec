# Lec9
# 
setwd("~/R/MOOC")
iris <- read.csv(file = "iris.csv")
head(iris)
str(iris)
attach(iris)

set.seed(1000)
N = nrow(iris)
tr.idx = sample(1:N, size = N*2/3, replace = FALSE)
tr.idx

iris.train <- iris[tr.idx, -5]
iris.test  <- iris[-tr.idx, -5]

trainLabels <- iris[tr.idx, 5]
testLabels  <- iris[-tr.idx, 5]

library(dplyr)
sample_n(iris, size = N*2/3, replace = FALSE)
sample_frac(iris, size = 0.6, replace = FALSE)



#############################################################################################################################

# Lec10
# Classification
# K-Nearest Neighbour

install.packages("class")
install.packages("gmodels")
install.packages("scales")
library(class)
library(gmodels)
library(scales)

# Load from week9
setwd("D:/R/MOOC/week10")
iris <- read.csv(file = "iris.csv")
head(iris)
str(iris)
attach(iris)

# Data Sampling
set.seed(1000, sample.kind = "Rounding")
N = nrow(iris)
tr.idx = sample(1:N, size = N*2/3, replace = FALSE)
tr.idx

iris.train <- iris[tr.idx, -5]
iris.test  <- iris[-tr.idx, -5]

trainLabels <- iris[tr.idx, 5]
testLabels  <- iris[-tr.idx, 5]

train<-iris[tr.idx,]
test<-iris[-tr.idx,]


# 10.1
# KNN with k = 5
md1 <- knn(train = iris.train, test = iris.test, cl = trainLabels, k = 5)
CrossTable(x = testLabels, y = md1, prop.chisq = FALSE)


# 10.2
# Optimal k selection (1 to n/2)
accuracy_k <- NULL
nnum <- nrow(iris.train)/2
for (kk in c(1:nnum)) {
  set.seed(1234)
  knn_k <- knn(train = iris.train, test = iris.test, cl = trainLabels, k = kk)
  accuracy_k <- c(accuracy_k, sum(knn_k == testLabels)/length(testLabels))
}

test_k <- data.frame(k = c(1:nnum), accuracy = accuracy_k[c(1:nnum)])
plot(formula = accuracy ~ k, data = test_k, type = "o", ylim = c(0.5, 1), pch = 20, col = 3)
with(test_k, text(accuracy ~ k, labels = k, pos = 1, cex = 0.7))

min(test_k[test_k$accuracy %in% max(accuracy_k), "k"])

# Final Model
md1 <- knn(train = iris.train, test = iris.test, cl = trainLabels, k = 7)
CrossTable(x = testLabels, y = md1, prop.chisq = FALSE)

plot(formula = Petal.Length ~ Petal.Width,
     data = iris.train,
     col = alpha(c("purple", "blue", "green"), 0.7)[trainLabels],
     main = "knn(k=7)")
points(formula = Petal.Length ~ Petal.Width,
       data = iris.test,
       pch = 17, cex = 1.2,
       col = alpha(c("purple", "blue", "green"), 0.7)[md1])     
legend("bottomright",
       c(paste("train", levels(trainLabels)), paste("test", levels(testLabels))),
       pch = c(rep(1, 3), rep(17, 3)),
       col = c(rep(alpha(c("purple", "blue", "green"), 0.7), 2)),
       cex = 0.9)

# ggplot - easier, prettier
install.packages("ggplot2")
library(ggplot2)

ggplot() +
  geom_point(data = iris.train, 
             aes(x = Petal.Width, y = Petal.Length, colour = trainLabels), size = 3) +
  geom_point(data = iris.test, 
             aes(x = Petal.Width, y = Petal.Length, fill = md1), shape = 25, size = 3) +
  labs(title = "KNN(k = 7)")
    

# Weighted KNN
install.packages("kknn")
library(kknn)

# knn(Weighted Value)
md2 <- kknn(Species ~ ., train = train, test = test,
            k = 5, distance = 1, kernel = "triangular")
md2
md2_fit <- fitted(md2)
md2_fit
CrossTable(x = testLabels, y = md2_fit,
           prop.chisq = FALSE, prop.c = FALSE)

md3 <- kknn(Species ~ ., train = train, test = test,
            k = 7, distance = 2, kernel = "triangular")
md3
md3_fit <- fitted(md3)
md3_fit
CrossTable(x = testLabels, y = md3_fit,
           prop.chisq = FALSE, prop.c = FALSE)


# Lec10
# Classification
# Discriminant Analysis


# 10.3
setwd("D:/R/MOOC/week10")
iris <- read.csv(file = "iris.csv")
attach(iris)

set.seed(1000, sample.kind = "Rounding")
N = nrow(iris)
tr.idx = sample(1:N, size = N*2/3, replace = FALSE)

iris.train <- iris[tr.idx, -5]
iris.test  <- iris[-tr.idx, -5]

trainLabels <- iris[tr.idx, 5]
testLabels  <- iris[-tr.idx, 5]

train<-iris[tr.idx,]
test<-iris[-tr.idx,]

# Linear Discriminant Analysis (LDA) with training data
install.packages("MASS")
library(MASS)
iris.lda <- lda(Species ~ ., data = train, prior = c(1/3, 1/3, 1/3))
iris.lda

# Predict test data set n = 50
testpred <- predict(iris.lda, test)
testpred

library(gmodels)
CrossTable(x = testLabels, y = testpred$class, prop.chisq = FALSE)

# Quiz
iris.lda2 <- lda(Species ~ ., data = train, prior = c(1/2, 1/4, 1/4))
iris.lda2
testpred2 <- predict(iris.lda2, test)
CrossTable(x = testLabels, y = testpred2$class, prop.chisq = FALSE)
install.packages("tidyr")
library(dplyr)
round(testpred2$posterior,2)


# 10.4
# 등분산 검정 (모집단의 분산-공분산 행렬이 동일한지 테스트)
install.packages("biotools")
library(biotools)
boxM(iris[1:4], iris$Species) # 귀무가설 기각 -> 등분산 아님 -> QDA

# Quadratic D Analysis
iris.qda <- qda(Species ~ ., data = train, prior = c(1/3, 1/3, 1/3))
iris.qda

testpredq <- predict(iris.qda, test)
testpredq

CrossTable(x = testLabels, y = testpredq$class, prop.chisq = FALSE)

# Partition Plot
install.packages("klaR")
library(klaR)
partimat(as.factor(iris$Species) ~ ., data = iris, method = "lda")
partimat(as.factor(iris$Species) ~ ., data = iris, method = "qda")


#########################################################################################################################

# Lec11
# Classification
# Support Vector Machine
install.packages("e1071")
library(e1071)

setwd("~/Desktop/R")
iris <- read.csv("iris.csv")
attach(iris)


# 11.1
# Support Vector Machine
# Use all data
m1 <- svm(Species ~ ., data = iris)
summary(m1)

# test
x    <- iris[, -5]
pred <- predict(m1, x)
y    <- iris[, 5]
table(pred, y)

# visualisation (optional, taking only 2 x's)
plot(m1, iris, Petal.Width ~ Petal.Length, slice=list(Sepal.Width=3, Sepal.Length=4))


# 11.2
install.packages("caret")
library(caret)

set.seed(1000, sample.kind = "Rounding")
N <- nrow(iris)
tr.idx = sample(1:N, size = N*2/3, replace = FALSE)
y <- iris[, 5]
train <- iris[tr.idx, ]
test  <- iris[-tr.idx, ]

# svm using kernel
m1 <- svm(Species ~ ., data = train, kernal = "radial")
summary(m1)
m2 <- svm(Species ~ ., data = train, kernel = "polynomial")
summary(m2)
m3 <- svm(Species ~ ., data = train, kernel = "sigmoid")
summary(m3)
m4 <- svm(Species ~ ., data = train, kernel = "linear")
summary(m4)

pred11 <- predict(m1, test)
confusionMatrix(pred11, test$Species)
pred12 <- predict(m2, test)
confusionMatrix(pred12, test$Species)
pred13 <- predict(m3, test)
confusionMatrix(pred13, test$Species)
pred14 <- predict(m4, test)
confusionMatrix(pred14, test$Species)

# 11.3
cancer <- read.csv("cancer.csv")
head(cancer, n = 10)
cancer <- cancer[, names(cancer) != "X1"]
attach(cancer)

# training / test
set.seed(1000)
N = nrow(cancer)
set.seed(998)
tr.idx = sample(1:N, size = N*2/3, replace = FALSE)
train <- cancer[ tr.idx, ]
test  <- cancer[-tr.idx, ]

# svm using kernel
m1 <- svm(Y ~ ., data = train, kernel = "radial")
summary(m1)
m2 <- svm(Y ~ ., data = train, kernel = "polynomial")
summary(m2)
m3 <- svm(Y ~ ., data = train, kernel = "sigmoid")
summary(m3)
m4 <- svm(Y ~ ., data = train, kernel = "linear")
summary(m4)

# test models
pred11 <- predict(m1, test)
confusionMatrix(pred11, test$Y)
pred12 <- predict(m2, test)
confusionMatrix(pred12, test$Y)
pred13 <- predict(m3, test)
confusionMatrix(pred13, test$Y)
pred14 <- predict(m4, test)
confusionMatrix(pred14, test$Y)
